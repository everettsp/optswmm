{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2758217a",
   "metadata": {},
   "source": [
    "# Simple SWMM Model Calibration Example\n",
    "\n",
    "This example demonstrates how to set up and run a basic calibration using optswmm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47754da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "# Import optswmm modules\n",
    "\n",
    "from optswmm.utils.calparams import CalParam, CalParams\n",
    "from optswmm.utils.calibutils import calibrate\n",
    "from optswmm.utils.optconfig import OptConfig\n",
    "from optswmm.utils.runutils import OptRun\n",
    "from swmmio import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786563ff",
   "metadata": {},
   "source": [
    "## 1. Set up file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360c2af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: c:\\Users\\everett\\Documents\\GitHub\\optswmm\\tests\\fixtures\\sample_models\\Example1.inp\n",
      "Config file: c:\\Users\\everett\\Documents\\GitHub\\optswmm\\tests\\fixtures\\configs\\sample_optconfig.yaml\n",
      "Test directory: c:\\Users\\everett\\Documents\\GitHub\\optswmm\\tests\\fixtures\n"
     ]
    }
   ],
   "source": [
    "# Define paths relative to the optswmm root directory\n",
    "optswmm_root = Path.cwd().parent.parent  # Assuming we're in examples folder\n",
    "test_dir = optswmm_root / \"tests\" / \"fixtures\"\n",
    "\n",
    "# Model file (you'll need to ensure this exists in your test fixtures)\n",
    "model_file = test_dir / \"sample_models\" / \"Example1.inp\"\n",
    "\n",
    "# Data files\n",
    "forcing_data = test_dir / \"sample_data\" / \"forcing_data.csv\"\n",
    "target_data = test_dir / \"sample_data\" / \"observed_data.csv\"\n",
    "\n",
    "# Configuration file\n",
    "config_file = test_dir / \"configs\" / \"sample_optconfig.yaml\"\n",
    "\n",
    "print(f\"Model file: {model_file}\")\n",
    "print(f\"Config file: {config_file}\")\n",
    "print(f\"Test directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263d4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... SWMM Version 5.2.4\n",
      "... Run Complete"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyswmm import Simulation, Output\n",
    "from swmm.toolkit import shared_enum\n",
    "\n",
    "from swmmio import Model\n",
    "model = Model(str(model_file))\n",
    "outfall_node = model.inp.outfalls.index[0]\n",
    "\n",
    "with Simulation(str(model_file)) as sim:\n",
    "    sim.execute()\n",
    "\n",
    "with Output(str(model_file.with_suffix(\".out\"))) as out:\n",
    "    res = out.node_series(outfall_node, shared_enum.NodeAttribute.TOTAL_INFLOW)\n",
    "out.close()\n",
    "df = pd.Series(res).sort_index().rename('flow').to_frame()\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.index.name = 'datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibration parameters\n",
    "cal_params = CalParams([\n",
    "    # Subcatchment parameters\n",
    "    CalParam(\n",
    "        section='subcatchments',\n",
    "        attribute='PercImperv',\n",
    "        lower_rel=-0.2,  # 20% decrease\n",
    "        upper_rel=0.3,   # 30% increase\n",
    "        distributed=True,\n",
    "    ),\n",
    "    CalParam(\n",
    "        section='subcatchments',\n",
    "        attribute='Width',\n",
    "        lower_rel=-0.15,\n",
    "        upper_rel=0.25,\n",
    "        distributed=True,\n",
    "    ),\n",
    "    # Conduit roughness\n",
    "    CalParam(\n",
    "        section='conduits',\n",
    "        attribute='Roughness',\n",
    "        lower_rel=-0.3,\n",
    "        upper_rel=0.5,\n",
    "        distributed=True,\n",
    "    )\n",
    "])\n",
    "\n",
    "model_vals = [10, 5, 1]\n",
    "for ii in range(len(cal_params)):\n",
    "    cal_params[ii].model_val = model_vals[ii]\n",
    "spc = CalParams.make_simulation_preconfig(model=model)\n",
    "    \n",
    "\n",
    "print(f\"Created {len(cal_params)} calibration parameters:\")\n",
    "for i, cp in enumerate(cal_params):\n",
    "    print(f\"  {i+1}. {cp.section}.{cp.attribute} (mode: {cp.mode}, distributed: {cp.distributed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bf135",
   "metadata": {},
   "source": [
    "## 2. Create sample data (if not exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d120e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create sample observed flow data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtarget_data\u001b[49m.exists():\n\u001b[32m      3\u001b[39m     target_data.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Generate synthetic observed flow data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'target_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Create sample observed flow data\n",
    "if not target_data.exists():\n",
    "    target_data.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate synthetic observed flow data\n",
    "    dates = pd.date_range('1998-01-01 00:00', periods=24, freq='H')\n",
    "    \n",
    "    # Create multi-level columns for nodes and variables\n",
    "    nodes = ['J1', 'J2', 'J3']\n",
    "    variables = ['flow']\n",
    "    \n",
    "    columns = pd.MultiIndex.from_product([variables, nodes], names=['variable', 'node'])\n",
    "    \n",
    "    # Generate synthetic flow data with some correlation to rainfall\n",
    "    forcing_df = pd.read_csv(forcing_data, index_col=0, parse_dates=True)\n",
    "    base_flow = 0.5  # m3/s base flow\n",
    "    \n",
    "    target_df = pd.DataFrame(index=dates, columns=columns)\n",
    "    \n",
    "    for node in nodes:\n",
    "        # Simple rainfall-runoff relationship\n",
    "        runoff = forcing_df['rainfall'] * 0.1 * np.random.uniform(0.8, 1.2)\n",
    "        flow = base_flow + runoff + np.random.normal(0, 0.1, size=48)\n",
    "        flow[flow < 0] = base_flow  # Ensure non-negative flows\n",
    "        target_df[('flow', node)] = flow\n",
    "    \n",
    "    target_df.to_csv(target_data)\n",
    "    print(f\"Created sample target data: {target_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e20844",
   "metadata": {},
   "source": [
    "## 3. Define calibration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda557fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 calibration parameters:\n",
      "  1. subcatchments.PercImperv (mode: multiplicative, distributed: True)\n",
      "  2. subcatchments.Width (mode: multiplicative, distributed: True)\n",
      "  3. conduits.Roughness (mode: multiplicative, distributed: True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60e5d02",
   "metadata": {},
   "source": [
    "## 4. Create optimization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56ed81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create optimization configuration\u001b[39;00m\n\u001b[32m      2\u001b[39m config_data = {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrun_folder\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[43mPath\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mruns\u001b[39m\u001b[33m\"\u001b[39m).resolve()),\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_file\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(model_file.resolve()),\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m#'forcing_data_file': str(forcing_data.resolve()),\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtarget_data_file\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(target_data.resolve()),\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33malgorithm\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mdifferential-evolution\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33malgorithm_options\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m      9\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m50\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpopsize\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m10\u001b[39m,\n\u001b[32m     11\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m42\u001b[39m\n\u001b[32m     12\u001b[39m     },\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscore_function\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mnse\u001b[39m\u001b[33m'\u001b[39m],  \u001b[38;5;66;03m# Nash-Sutcliffe Efficiency\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtarget_variables\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mflow\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhierarchical\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnormalize\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwarmup_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     18\u001b[39m }\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Save configuration to file\u001b[39;00m\n\u001b[32m     21\u001b[39m config_file.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Create optimization configuration\n",
    "config_data = {\n",
    "    'run_folder': str(Path(\"runs\").resolve()),\n",
    "    'model_file': str(model_file.resolve()),\n",
    "    #'forcing_data_file': str(forcing_data.resolve()),\n",
    "    'target_data_file': str(target_data.resolve()),\n",
    "    'algorithm': 'differential-evolution',\n",
    "    'algorithm_options': {\n",
    "        'maxiter': 50,\n",
    "        'popsize': 10,\n",
    "        'seed': 42\n",
    "    },\n",
    "    'score_function': ['nse'],  # Nash-Sutcliffe Efficiency\n",
    "    'target_variables': ['flow'],\n",
    "    'hierarchical': False,\n",
    "    'normalize': False,\n",
    "    'warmup_length': 0,\n",
    "}\n",
    "\n",
    "# Save configuration to file\n",
    "config_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "import yaml\n",
    "\n",
    "with open(config_file, 'w') as f:\n",
    "    yaml.dump(config_data, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Created optimization configuration: {config_file}\")\n",
    "print(f\"Algorithm: {config_data['algorithm']}\")\n",
    "print(f\"Max iterations: {config_data['algorithm_options']['maxiter']}\")\n",
    "print(f\"Score function: {config_data['score_function']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a7ae3",
   "metadata": {},
   "source": [
    "## 5. Load and preprocess calibration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimization configuration\n",
    "opt_config = OptConfig(config_file=config_file)\n",
    "\n",
    "# Preprocess calibration parameters\n",
    "cal_params_processed = cal_params.preprocess(opt_config)\n",
    "\n",
    "print(f\"Processed {len(cal_params_processed)} calibration parameters\")\n",
    "print(f\"Total active parameters: {sum(1 for cp in cal_params_processed if cp.active)}\")\n",
    "\n",
    "# Display parameter bounds\n",
    "bounds = cal_params_processed.get_bounds()\n",
    "flattened_bounds = cal_params_processed.flatten_bounds(bounds)\n",
    "print(f\"\\nParameter bounds (flattened): {len(flattened_bounds)} parameters\")\n",
    "for i, bound in enumerate(flattened_bounds[:5]):  # Show first 5\n",
    "    print(f\"  Parameter {i+1}: {bound}\")\n",
    "if len(flattened_bounds) > 5:\n",
    "    print(f\"  ... and {len(flattened_bounds) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157405b5",
   "metadata": {},
   "source": [
    "## 6. Run the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calibration...\n",
      "This may take several minutes depending on model complexity and iteration count.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\everett\\Documents\\GitHub\\optswmm\\optswmm\\utils\\standardization.py:215: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(file, index_col=0, parse_dates=True)\n",
      "c:\\Users\\everett\\Documents\\GitHub\\optswmm\\optswmm\\utils\\standardization.py:169: UserWarning: Target data 'Index(['timestamp', 'value'], dtype='object')' has less than 1000 timesteps\n",
      "  warnings.warn(f\"Target data '{tgt.columns}' has less than 1000 timesteps\", UserWarning)\n",
      "c:\\Users\\everett\\Documents\\GitHub\\optswmm\\optswmm\\utils\\optconfig.py:111: UserWarning: Uncalibrated model file C:\\Users\\everett\\Documents\\GitHub\\optswmm\\tests\\fixtures\\sample_models\\Example1_uncal.inp already exists. It will be overwritten.\n",
      "  warnings.warn(f\"Uncalibrated model file {uncalibrated_model_file} already exists. It will be overwritten.\")\n",
      "c:\\Users\\everett\\Documents\\GitHub\\optswmm\\optswmm\\utils\\standardization.py:215: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(file, index_col=0, parse_dates=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Calibration failed with error: \n",
      "  ERROR 191: simulation start date comes after ending date.\n",
      "This might be due to missing model file or data incompatibility.\n",
      "Please check that the model file exists and is compatible with the test data.\n"
     ]
    }
   ],
   "source": [
    "# Run calibration\n",
    "print(\"Starting calibration...\")\n",
    "print(\"This may take several minutes depending on model complexity and iteration count.\")\n",
    "\n",
    "try:\n",
    "    calibrate(opt_config=opt_config, cal_params=cal_params_processed)\n",
    "    print(\"\\n‚úÖ Calibration completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Calibration failed with error: {e}\")\n",
    "    print(\"This might be due to missing model file or data incompatibility.\")\n",
    "    print(\"Please check that the model file exists and is compatible with the test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5a7c6",
   "metadata": {},
   "source": [
    "## 7. Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load optimization results: OptRun.__init__() got an unexpected keyword argument 'config_file'\n",
      "This is normal if the calibration failed or is still running.\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze optimization results\n",
    "try:\n",
    "    opt_run = OptRun(config_file=config_file)\n",
    "    \n",
    "    print(f\"Results directory: {opt_run.dir}\")\n",
    "    print(f\"Config loaded from: {opt_run.config_file}\")\n",
    "    \n",
    "    # Check if results files exist\n",
    "    results_files = {\n",
    "        'Parameters': opt_run.dir / 'results_params.txt',\n",
    "        'Scores': opt_run.dir / 'results_scores.txt',\n",
    "        'Calibration Parameters': opt_run.dir / 'calibration_parameters.csv',\n",
    "        'Calibrated Model': opt_run.dir / 'calibrated_model.inp'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nResults files:\")\n",
    "    for name, file_path in results_files.items():\n",
    "        exists = \"‚úÖ\" if file_path.exists() else \"‚ùå\"\n",
    "        print(f\"  {exists} {name}: {file_path.name}\")\n",
    "    \n",
    "    # Load and display optimization scores if available\n",
    "    scores_file = results_files['Scores']\n",
    "    if scores_file.exists():\n",
    "        scores_df = pd.read_csv(scores_file)\n",
    "        print(f\"\\nOptimization progress:\")\n",
    "        print(f\"  Total iterations: {len(scores_df)}\")\n",
    "        print(f\"  Best score: {scores_df['score'].min():.4f}\")\n",
    "        print(f\"  Final score: {scores_df['score'].iloc[-1]:.4f}\")\n",
    "        \n",
    "        # Show score improvement\n",
    "        initial_score = scores_df['score'].iloc[0]\n",
    "        final_score = scores_df['score'].min()\n",
    "        improvement = ((initial_score - final_score) / abs(initial_score)) * 100\n",
    "        print(f\"  Improvement: {improvement:.1f}%\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load optimization results: {e}\")\n",
    "    print(\"This is normal if the calibration failed or is still running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19358688",
   "metadata": {},
   "source": [
    "## 8. Plot results (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d372b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating plots: name 'opt_run' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Plot optimization progress\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Load results\n",
    "    scores_file = opt_run.dir / 'results_scores.txt'\n",
    "    params_file = opt_run.dir / 'results_params.txt'\n",
    "    \n",
    "    if scores_file.exists() and params_file.exists():\n",
    "        scores_df = pd.read_csv(scores_file)\n",
    "        params_df = pd.read_csv(params_file)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "        \n",
    "        # Plot score evolution\n",
    "        ax1.plot(scores_df['iter'], scores_df['score'], 'b-', linewidth=2)\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_ylabel('Objective Function Value')\n",
    "        ax1.set_title('Optimization Progress')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot parameter evolution (first few parameters)\n",
    "        param_cols = [col for col in params_df.columns if col not in ['iter', 'score']]\n",
    "        for i, col in enumerate(param_cols[:5]):  # Plot first 5 parameters\n",
    "            ax2.plot(params_df['iter'], params_df[col], label=f'Param {i+1}', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Parameter Value')\n",
    "        ax2.set_title('Parameter Evolution')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"üìä Optimization plots generated successfully!\")\n",
    "    else:\n",
    "        print(\"Results files not found - cannot generate plots\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available - skipping plots\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating plots: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8ec86",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b5804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CALIBRATION EXAMPLE SUMMARY\n",
      "==================================================\n",
      "Model file: Example1.inp\n",
      "Configuration: sample_optconfig.yaml\n",
      "Calibration parameters: 3\n",
      "Algorithm: differential-evolution\n",
      "Score function: nse\n",
      "\n",
      "No results directory found - calibration may have failed\n",
      "\n",
      "==================================================\n",
      "Example completed!\n",
      "\n",
      "Next steps:\n",
      "- Examine the calibrated model file\n",
      "- Analyze parameter sensitivity\n",
      "- Validate results with independent data\n",
      "- Adjust calibration settings if needed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CALIBRATION EXAMPLE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model file: {model_file.name}\")\n",
    "print(f\"Configuration: {config_file.name}\")\n",
    "print(f\"Calibration parameters: {len(cal_params)}\")\n",
    "print(f\"Algorithm: {config_data['algorithm']}\")\n",
    "print(f\"Score function: {config_data['score_function'][0]}\")\n",
    "\n",
    "try:\n",
    "    if opt_run.dir.exists():\n",
    "        print(f\"\\nResults saved to: {opt_run.dir}\")\n",
    "        \n",
    "        # List key output files\n",
    "        key_files = [\n",
    "            'calibrated_model.inp',\n",
    "            'results_scores.txt', \n",
    "            'results_params.txt',\n",
    "            'calibration_parameters.csv'\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nKey output files:\")\n",
    "        for filename in key_files:\n",
    "            filepath = opt_run.dir / filename\n",
    "            status = \"‚úÖ\" if filepath.exists() else \"‚ùå\"\n",
    "            print(f\"  {status} {filename}\")\n",
    "            \n",
    "except:\n",
    "    print(\"\\nNo results directory found - calibration may have failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Example completed!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Examine the calibrated model file\")\n",
    "print(\"- Analyze parameter sensitivity\")\n",
    "print(\"- Validate results with independent data\")\n",
    "print(\"- Adjust calibration settings if needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
